#ä¿®å¥½å»æ‰è‡ªå®šç¾©log exporter classçš„å•é¡Œ
#çœ‹pdfä¾†å­¸ç¿’&äº†è§£ä¿®æ”¹çš„éç¨‹
#ä¸å«å‹•æ…‹label
import csv
import os
import time
import logging
from prometheus_client import Gauge, start_http_server
from threading import Lock
from logging.handlers import RotatingFileHandler

# è¨­ç½®æ—¥èªŒè¼ªæ›¿
log_handler = RotatingFileHandler(
    "exporter.log", maxBytes=5 * 1024 * 1024, backupCount=3
)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[log_handler]
)

# **ä¿®æ­£ `Gauge` çš„ `labels` å®šç¾©æ–¹å¼**
log_host_job_count = Gauge("log_host_job_count", "Count of occurrences", ["host", "job_name"])

# è³‡æ–™å¿«å–èˆ‡ Scraper è¨˜éŒ„
metric_cache = {}
cache_lock = Lock()
scraper_access_record = {}  # è¨˜éŒ„ Scraper æ˜¯å¦å·²æŠ“å–


def collect(scraper_ip):
    """ç¢ºä¿ Scraper åœ¨ `metric` æ›´æ–°é€±æœŸå…§åªèƒ½æŠ“å–ä¸€æ¬¡"""
    with cache_lock:
        if scraper_ip in scraper_access_record:
            logging.warning(f"Scraper {scraper_ip} å·²ç¶“æŠ“å–éï¼Œæ‹’çµ•æä¾›æ•¸æ“š")
            return

        scraper_access_record[scraper_ip] = time.time()

        # **æ¸…ç©ºèˆŠçš„ metrics**
        log_host_job_count._metrics.clear()

        # **æ›´æ–° Prometheus æŒ‡æ¨™**
        for (host, job_name), count in metric_cache.items():
            log_host_job_count.labels(host=host, job_name=job_name).set(count)
            print(f"[DEBUG] è¨­å®š `metrics` => host={host}, job_name={job_name}, count={count}")

        logging.info("Updated metrics successfully.")
        print("[DEBUG] Updated metrics successfully.")  # ğŸ” Debug


def update_metrics():
    """å¾æœ€æ–°çš„ data_collect.csv æ›´æ–° metric"""
    log_file = "data_collect.csv"

    if not os.path.exists(log_file):
        logging.warning(f"Log file {log_file} does not exist.")
        print("[DEBUG] Log file does not exist.")  # ğŸ” Debug
        return

    counts = {}
    try:
        with open(log_file, 'r') as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) < 2:  # ç¢ºä¿è‡³å°‘æœ‰ `host, job_name`
                    logging.warning(f"Invalid row format: {row}")
                    print(f"[DEBUG] Invalid row format: {row}")  # ğŸ” Debug
                    continue

                host, job_name = row[0].strip(), row[1].strip()
                key = (host, job_name)
                counts[key] = counts.get(key, 0) + 1

    except Exception as e:
        logging.error(f"Error reading log file {log_file}: {e}")
        print(f"[DEBUG] Error reading log file {log_file}: {e}")  # ğŸ” Debug
        return

    with cache_lock:
        global metric_cache
        metric_cache = counts
        scraper_access_record.clear()

        # **æ‰“å° metrics è¨ˆç®—çµæœ**
        print("\n[DEBUG] Updated metric_cache:")
        for key, value in metric_cache.items():
            print(f"[DEBUG] {key} : {value}")

    logging.info("Metrics updated successfully.")
    print("[DEBUG] Metrics updated successfully.")  # ğŸ” Debug


if __name__ == "__main__":
    # å•Ÿå‹• Prometheus HTTP ä¼ºæœå™¨
    start_http_server(8080)
    logging.info("Prometheus exporter running on http://localhost:8080/metrics")
    print("Prometheus exporter running on http://localhost:8080/metrics")  # ğŸ” Debug

    # ç›£æ§è¿´åœˆ
    while True:
        update_metrics()
        collect("localhost")  # **æ‰‹å‹•è§¸ç™¼ collect**
        time.sleep(10)
