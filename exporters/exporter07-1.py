import csv
import os
import time
import logging
from datetime import datetime
from prometheus_client import Gauge, start_http_server
from threading import Lock
from logging.handlers import RotatingFileHandler

# è¨­ç½®æ—¥èªŒè¼ªæ›¿
log_handler = RotatingFileHandler(
    "exporter.log", maxBytes=5 * 1024 * 1024, backupCount=3
)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[log_handler]
)

# å®šç¾© Prometheus æŒ‡æ¨™
log_host_job_count = Gauge("log_host_job_count", "Count of occurrences of host and job_name in log")

# è³‡æ–™å¿«å–èˆ‡ Scraper è¨˜éŒ„
metric_cache = {}
cache_lock = Lock()
scraper_access_record = {}  # è¨˜éŒ„ Scraper æ˜¯å¦å·²æŠ“å–


def collect(scraper_ip):
    """ç¢ºä¿ Scraper åœ¨ `metric` æ›´æ–°é€±æœŸå…§åªèƒ½æŠ“å–ä¸€æ¬¡"""
    with cache_lock:
        if scraper_ip in scraper_access_record:
            logging.warning(f"Scraper {scraper_ip} å·²ç¶“æŠ“å–éï¼Œæ‹’çµ•æä¾›æ•¸æ“š")
            return

        scraper_access_record[scraper_ip] = time.time()

        # æ›´æ–° Prometheus æŒ‡æ¨™
        total_count = sum(metric_cache.values())  # è¨ˆç®—æ‰€æœ‰ host/job_name å‡ºç¾çš„ç¸½æ•¸
        log_host_job_count.set(total_count)

        logging.info(f"Updated metrics: log_host_job_count = {total_count}")
        print(f"[DEBUG] Updated metrics: log_host_job_count = {total_count}")  # ğŸ” Debug


def update_metrics():
    """å¾æœ€æ–°çš„ data_collect.csv æ›´æ–° metric"""
    log_file = "data_collect.csv"

    if not os.path.exists(log_file):
        logging.warning(f"Log file {log_file} does not exist.")
        print("[DEBUG] Log file does not exist.")  # ğŸ” Debug
        return

    counts = {}
    try:
        with open(log_file, 'r') as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) < 2:  # ç¢ºä¿è‡³å°‘æœ‰ `host, job_name`
                    logging.warning(f"Invalid row format: {row}")
                    print(f"[DEBUG] Invalid row format: {row}")  # ğŸ” Debug
                    continue

                host, job_name = row[0].strip(), row[1].strip()
                key = (host, job_name)
                counts[key] = counts.get(key, 0) + 1

    except Exception as e:
        logging.error(f"Error reading log file {log_file}: {e}")
        print(f"[DEBUG] Error reading log file {log_file}: {e}")  # ğŸ” Debug
        return

    with cache_lock:
        global metric_cache
        metric_cache = counts
        scraper_access_record.clear()

        # æ‰“å° metrics è¨ˆç®—çµæœ
        print("\n[DEBUG] Updated metric_cache:")
        for key, value in metric_cache.items():
            print(f"[DEBUG] {key} : {value}")

    logging.info("Metrics updated successfully.")
    print("[DEBUG] Metrics updated successfully.")  # ğŸ” Debug


if __name__ == "__main__":
    # å•Ÿå‹• Prometheus HTTP ä¼ºæœå™¨
    start_http_server(8080)
    logging.info("Prometheus exporter running on http://localhost:8080/metrics")
    print("Prometheus exporter running on http://localhost:8080/metrics")  # ğŸ” Debug

    # ç›£æ§è¿´åœˆ
    while True:
        update_metrics()
        time.sleep(10)
